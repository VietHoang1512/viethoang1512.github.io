
@article{phan2024controllable,
  abbr = {ICML},
  title={Controllable Prompt Tuning For Balancing Group Distributional Robustness},
  author={Phan, Hoang and Wilson, Andrew Gordon and Lei, Qi},
  journal={Proceedings of the 41st International Conference on Machine Learning},
  code={https://github.com/VietHoang1512/CPT},
  pdf={https://arxiv.org/abs/2403.02695},
  abstract={Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring only 0.4\% tunable parameters.},
  published={true},
  year={2024}
}



@article{nguyen2023flat,
  abbr = {NeurIPS},
  title={Flat Seeking Bayesian Neural Networks},
  author={Nguyen, Anh and Vuong, Long and Phan, Hoang and Do, Toan and Phung, Dinh and Le, Trung},
  selected = {true},
  journal={Advances in Neural Information Processing Systems},
  code={https://github.com/anh-ntv/flat_bnn},
  published={true},
  pdf={https://arxiv.org/abs/2302.02713},
  abstract={Bayesian Neural Networks (BNNs) offer a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferencing a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with a lower sharpness have a better generalization ability. Nonetheless, existing posterior inferences are not aware of sharpness/flatness, hence possibly leading to high sharpness for the models sampled from it. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior and the optimal approximate posterior estimating this sharpness-aware posterior have a better flatness, hence possibly possessing a higher generalization ability. We conduct experiments by leveraging the sharpness-aware posterior with the state-of-the-art Bayesian Neural Networks, showing that the flat-seeking counterparts outperform their baselines in all metrics of interest.},
  year={2023}
}






@article{phan2022global,
  abbr = {AISTATS},
  title={Global-Local Regularization Via Distributional Robustness},
  author={Phan, Hoang and Le, Trung and Phung, Trung and Bui, Tuan Anh and Ho, Nhat and Phung, Dinh},
  journal={Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  selected = {true},
  published={true},
  code={https://github.com/VietHoang1512/GLOT},
  pdf={https://arxiv.org/abs/2203.00553},
  abstract={Despite superior performance in many situations, deep neural networks are often vulnerable to adversarial examples and distribution shifts, limiting model generalization ability in real-world applications. To alleviate these problems, recent approaches leverage distributional robustness optimization (DRO) to find the most challenging distribution, and then minimize loss function over this most challenging distribution. Regardless of achieving some improvements, these DRO approaches have some obvious limitations. First, they purely focus on local regularization to strengthen model robustness, missing a global regularization effect which is useful in many real-world applications (e.g., domain adaptation, domain generalization, and adversarial machine learning). Second, the loss functions in the existing DRO approaches operate in only the most challenging distribution, hence decouple with the original distribution, leading to a restrictive modeling capability. In this paper, we propose a novel regularization technique, following the veins of Wasserstein-based DRO framework. Specifically, we define a particular joint distribution and Wasserstein-based uncertainty, allowing us to couple the original and most challenging distributions for enhancing modeling capability and applying both local and global regularizations. Empirical studies on different learning problems demonstrate that our proposed approach significantly outperforms the existing regularization approaches in various domains: semi-supervised learning, domain adaptation, domain generalization, and adversarial machine learning.},
  year={2023}
}




@article{phan2022stochastic,
  abbr = {NeurIPS},
  title={Stochastic Multiple Target Sampling Gradient Descent},
  author={Phan, Hoang and Tran, Ngoc and Le, Trung and Tran, Toan and Ho, Nhat and Phung, Dinh},
  journal={Advances in Neural Information Processing Systems },
  pdf={https://arxiv.org/abs/2206.01934},
  code={https://github.com/VietHoang1512/MT-SGD},
  abstract={Sampling from an unnormalized target distribution is an essential problem with many applications in probabilistic inference. Stein Variational Gradient Descent (SVGD) has been shown to be a powerful method that iteratively updates a set of particles to approximate the distribution of interest. Furthermore, when analysing its asymptotic properties, SVGD reduces exactly to a single-objective optimization problem and can be viewed as a probabilistic version of this single-objective optimization problem. A natural question then arises: "Can we derive a probabilistic version of the multi-objective optimization?". To answer this question, we propose Stochastic Multiple Target Sampling Gradient Descent (MT-SGD), enabling us to sample from multiple unnormalized target distributions. Specifically, our MT-SGD conducts a flow of intermediate distributions gradually orienting to multiple target distributions, which allows the sampled particles to move to the joint high-likelihood region of the target distributions. Interestingly, the asymptotic analysis shows that our approach reduces exactly to the multiple-gradient descent algorithm for multi-objective optimization, as expected. Finally, we conduct comprehensive experiments to demonstrate the merit of our approach to multi-task learning.},
  selected = {true},
  published={true},
  year={2022}
}

@article{phan2022reducing,
  abbr = {PAKDD},
  title={Reducing Catastrophic Forgetting in Neural Networks via Gaussian Mixture Approximation},
  author={Phan*, Hoang and Tuan Anh*, Phan and Nguyen*, Son and Linh, Ngo Van and Than, Khoat},
  journal={Advances in Knowledge Discovery and Data Mining. PAKDD. Lecture Notes in Computer Science. Springer },
  selected = {true},
  published={true},
  pdf={PAKDD_paper.pdf},
  code={https://github.com/anhpt1997/ucb_cl},
  abstract={Our paper studies the continual learning (CL) problems in which data comes in sequence and the trained models are expected to be capable of utilizing existing knowledge to solve new tasks without losing performance on previous ones. This also poses a central difficulty in the field of CL, termed as Catastrophic Forgetting (CF). In an attempt to address this problem, Bayesian methods provide a powerful principle, focusing on the inference scheme to estimate the importance of weights. Variational inference (VI), one of the most widely used methods within this vein, approximates the intractable posterior by a factorized distribution, thus offering computational efficiency. Notwithstanding many state-of-the-art performances in practice, this simple assumption about the posterior distribution typically limits the model capacity to some extent. In this paper, we introduce a novel approach to mitigate forgetting in the Bayesian approach via enriching the posterior distribution with mixture models, which intuitively promotes neural networks to acquire knowledge from multiple tasks at a time. Moreover, in order to reduce the model’s complexity growth when the number of components increases, we propose a solution that conducts low-rank decomposition on the variance of each component based on neural matrix factorization. Extensive experiments show that our method yields significant improvements compared to prior works on different benchmarks.},
  pages={106--117},
  year={2022},
  organization={Springer}
}

@article{phan2021matching,
  abbr = {EMNLP},
  title={Matching the statements: A simple and accurate model for key point analysis},
  author={Phan, Hoang and Nguyen, Long and Doan, Khanh},
  journal={Proceedings of the 8th Workshop on Argument Mining },
  selected = {true},
  pdf={https://aclanthology.org/2021.argmining-1.17/},
  code={https://github.com/VietHoang1512/KPA},
  abstract={Key Point Analysis (KPA) is one of the most essential tasks in building an Opinion Summarization system, which is capable of generating key points for a collection of arguments toward a particular topic. Furthermore, KPA allows quantifying the coverage of each summary by counting its matched arguments. With the aim of creating high-quality summaries, it is necessary to have an in-depth understanding of each individual argument as well as its universal semantic in a specified context. In this paper, we introduce a promising model, named Matching the Statements (MTS) that incorporates the discussed topic information into arguments/key points comprehension to fully understand their meanings, thus accurately performing ranking and retrieving best-match key points for an input argument. Our approach has achieved the 4th place in Track 1 of the Quantitative Summarization – Key Point Analysis Shared Task by IBM, yielding a competitive performance of 0.8956 (3rd) and 0.9632 (7th) strict and relaxed mean Average Precision, respectively.},
  published={true},
  pages={165--174},
  year={2021}
  }

@article{nguyen2021contrastive,
  abbr = {CVPR},
  title={Contrastive learning for natural language-based vehicle retrieval},
  author={Nguyen, Tam and Pham, Quang and Doan, Linh  and Trinh, Hoang  and Nguyen, Anh and Phan, Hoang},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops },
  selected = {true},
  published={true},
  pdf={https://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Nguyen_Contrastive_Learning_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2021_paper.html},
  code={https://github.com/VietHoang1512/CVPR-track-5},
  abstract={AI City Challenge 2021 Task 5: The Natural Language-Based Vehicle Tracking is a Natural Language-based Vehicle Retrieval task, which requires retrieving a single-camera track using a set of three natural language descriptions of the specific targets. In this paper, we present our methods to tackle the difficulties of the provided task. Experiments with our approaches on the competitive dataset from AICity Challenge 2021 show that our techniques achieve Mean Reciprocal Rank score of 0.1701 on the public test dataset and 0.1571 on the private test dataset.},
  pages={4245--4252},
  year={2021}
  }
